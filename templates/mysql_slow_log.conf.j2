
# Adding MySQL Slow Query Logs to Logstash
# https://www.phase2technology.com/blog/adding-mysql-slow-query-logs-to-logstash/

# How To Use MySQL Query Profiling
# https://www.digitalocean.com/community/tutorials/how-to-use-mysql-query-profiling

# Grok Debugger
# http://grokdebug.herokuapp.com/

input {
  file {
    type => "mysql-slow"
    path => "{{ logstash_mysql_slow_path }}"
    start_position => "beginning"

    # Key breaking the log up on the # User@Host line, this will mean
    # sometimes a # Time line from the next entry will be incorrectly
    # included but since that isn't consistently present it can't be
    # keyed off of
    #
    # Due to the way the multiline codec works, previous must be used
    # to collect everything which isn't the User line up. Since
    # queries can be multiline the User line can't be pushed forward
    # as it would only collect the first line of the actual query
    # data.
    #
    # logstash will always be one slow query behind because it needs
    # the User line to trigger that it is done with the previous entry.
    # A periodic "SELECT SLEEP(1);" where 1 is above the slow query
    # threshold can be used to "flush" these events through at the
    # expense of having spurious log entries (see the drop filter)
    codec => multiline {
      pattern => "^# User@Host:"
      negate => true
      what => previous
    }
  }
}

filter {
  # Capture user, optional host and optional ip fields
  # locate grok-patterns !!
  # sample log file lines:
  # User@Host: logstash[logstash] @ localhost [127.0.0.1]
  # User@Host: logstash[logstash] @  [127.0.0.1]
  grok {
    match => [ "message", "^#\s*User@Host:\s*%{USER:user}(?:[a-zA-Z0-9\[\]]+])?\s*@\s*%{IPORHOST:host}?\s*\[%{IP:ip}?\]\s*(Id:\s*)?%{INT}?" ]
  }
  # Capture query time, lock time, rows returned and rows examined
  # sample log file lines:
  # Query_time: 102.413328  Lock_time: 0.000167 Rows_sent: 0  Rows_examined: 1970
  # Query_time: 1.113464  Lock_time: 0.000128 Rows_sent: 1  Rows_examined: 0
  grok {
    match => [ "message", "^#\s*Query_time:\s*%{NUMBER:query_time:float}\s*Lock_time:\s*%{NUMBER:lock_time:float}\s*Rows_sent:\s*%{NUMBER:rows_sent:int}\s*Rows_examined:\s*%{NUMBER:rows_examined:int}"]
  }
  # Capture the time the query happened
  grok {
    match => [ "message", "^SET\s*timestamp=%{NUMBER:timestamp};" ]
  }
  # Extract the time based on the time of the query and not the time the item got logged
  # https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html
  # The date filter is used for parsing dates from fields, and then using that date or
  # timestamp as the logstash timestamp for the event.
  date {
    match => [ "timestamp", "UNIX" ]
  }
  # Drop the captured timestamp field since it has been moved to the time of the event
  # See date filter: "Store the matching timestamp into the given target field.
  # If not provided, default to updating the @timestamp field of the event."
  mutate {
    remove_field => "timestamp"
  }
}

output {
  # stdout { codec => rubydebug }
  elasticsearch {
    index => "logstash-mysql-slow-log-%{+YYYY.MM.dd}"
  }
}
